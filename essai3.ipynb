{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68fd22ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0e91450",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('quotes-2020.json.bz2', compression='bz2', nrows=100000, lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd4fdefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_quotes = df.quotation.apply(gensim.utils.simple_preprocess)\n",
    "tagged_data_2 = [TaggedDocument(words=doc, tags=[i]) for i, doc in enumerate(vocab_quotes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d3629d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On définit le modèle, il y a beaucoup de paramètres à étudier pour optimiser\n",
    "model = gensim.models.Doc2Vec(vector_size=100, dbow_words= 1, dm=0,  window=5, seed=1337, min_count=5, workers=4,alpha=0.025, min_alpha=0.025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5493310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On construit le vocab nécessaire au training \n",
    "model.build_vocab(tagged_data_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6db2369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On entraine le modèle\n",
    "model.train(tagged_data_2, total_examples=model.corpus_count, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a81e8bb7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk with 100000 rows\n",
      "Index(['quoteID', 'quotation', 'speaker', 'qids', 'date', 'numOccurrences',\n",
      "       'probas', 'urls', 'phase'],\n",
      "      dtype='object')\n",
      "Processing chunk with 100000 rows\n",
      "Index(['quoteID', 'quotation', 'speaker', 'qids', 'date', 'numOccurrences',\n",
      "       'probas', 'urls', 'phase'],\n",
      "      dtype='object')\n",
      "Processing chunk with 100000 rows\n",
      "Index(['quoteID', 'quotation', 'speaker', 'qids', 'date', 'numOccurrences',\n",
      "       'probas', 'urls', 'phase'],\n",
      "      dtype='object')\n",
      "Processing chunk with 100000 rows\n",
      "Index(['quoteID', 'quotation', 'speaker', 'qids', 'date', 'numOccurrences',\n",
      "       'probas', 'urls', 'phase'],\n",
      "      dtype='object')\n",
      "Processing chunk with 100000 rows\n",
      "Index(['quoteID', 'quotation', 'speaker', 'qids', 'date', 'numOccurrences',\n",
      "       'probas', 'urls', 'phase'],\n",
      "      dtype='object')\n",
      "Processing chunk with 100000 rows\n",
      "Index(['quoteID', 'quotation', 'speaker', 'qids', 'date', 'numOccurrences',\n",
      "       'probas', 'urls', 'phase'],\n",
      "      dtype='object')\n",
      "Processing chunk with 100000 rows\n",
      "Index(['quoteID', 'quotation', 'speaker', 'qids', 'date', 'numOccurrences',\n",
      "       'probas', 'urls', 'phase'],\n",
      "      dtype='object')\n",
      "Processing chunk with 100000 rows\n",
      "Index(['quoteID', 'quotation', 'speaker', 'qids', 'date', 'numOccurrences',\n",
      "       'probas', 'urls', 'phase'],\n",
      "      dtype='object')\n",
      "Processing chunk with 100000 rows\n",
      "Index(['quoteID', 'quotation', 'speaker', 'qids', 'date', 'numOccurrences',\n",
      "       'probas', 'urls', 'phase'],\n",
      "      dtype='object')\n",
      "Processing chunk with 100000 rows\n",
      "Index(['quoteID', 'quotation', 'speaker', 'qids', 'date', 'numOccurrences',\n",
      "       'probas', 'urls', 'phase'],\n",
      "      dtype='object')\n",
      "Processing chunk with 100000 rows\n",
      "Index(['quoteID', 'quotation', 'speaker', 'qids', 'date', 'numOccurrences',\n",
      "       'probas', 'urls', 'phase'],\n",
      "      dtype='object')\n",
      "Processing chunk with 100000 rows\n",
      "Index(['quoteID', 'quotation', 'speaker', 'qids', 'date', 'numOccurrences',\n",
      "       'probas', 'urls', 'phase'],\n",
      "      dtype='object')\n",
      "Processing chunk with 100000 rows\n",
      "Index(['quoteID', 'quotation', 'speaker', 'qids', 'date', 'numOccurrences',\n",
      "       'probas', 'urls', 'phase'],\n",
      "      dtype='object')\n",
      "Processing chunk with 100000 rows\n",
      "Index(['quoteID', 'quotation', 'speaker', 'qids', 'date', 'numOccurrences',\n",
      "       'probas', 'urls', 'phase'],\n",
      "      dtype='object')\n",
      "Processing chunk with 100000 rows\n",
      "Index(['quoteID', 'quotation', 'speaker', 'qids', 'date', 'numOccurrences',\n",
      "       'probas', 'urls', 'phase'],\n",
      "      dtype='object')\n",
      "Processing chunk with 100000 rows\n",
      "Index(['quoteID', 'quotation', 'speaker', 'qids', 'date', 'numOccurrences',\n",
      "       'probas', 'urls', 'phase'],\n",
      "      dtype='object')\n",
      "Processing chunk with 100000 rows\n",
      "Index(['quoteID', 'quotation', 'speaker', 'qids', 'date', 'numOccurrences',\n",
      "       'probas', 'urls', 'phase'],\n",
      "      dtype='object')\n",
      "Processing chunk with 100000 rows\n",
      "Index(['quoteID', 'quotation', 'speaker', 'qids', 'date', 'numOccurrences',\n",
      "       'probas', 'urls', 'phase'],\n",
      "      dtype='object')\n",
      "Processing chunk with 100000 rows\n",
      "Index(['quoteID', 'quotation', 'speaker', 'qids', 'date', 'numOccurrences',\n",
      "       'probas', 'urls', 'phase'],\n",
      "      dtype='object')\n",
      "Processing chunk with 100000 rows\n",
      "Index(['quoteID', 'quotation', 'speaker', 'qids', 'date', 'numOccurrences',\n",
      "       'probas', 'urls', 'phase'],\n",
      "      dtype='object')\n",
      "Processing chunk with 100000 rows\n",
      "Index(['quoteID', 'quotation', 'speaker', 'qids', 'date', 'numOccurrences',\n",
      "       'probas', 'urls', 'phase'],\n",
      "      dtype='object')\n",
      "Processing chunk with 100000 rows\n",
      "Index(['quoteID', 'quotation', 'speaker', 'qids', 'date', 'numOccurrences',\n",
      "       'probas', 'urls', 'phase'],\n",
      "      dtype='object')\n",
      "Processing chunk with 100000 rows\n",
      "Index(['quoteID', 'quotation', 'speaker', 'qids', 'date', 'numOccurrences',\n",
      "       'probas', 'urls', 'phase'],\n",
      "      dtype='object')\n",
      "Processing chunk with 100000 rows\n",
      "Index(['quoteID', 'quotation', 'speaker', 'qids', 'date', 'numOccurrences',\n",
      "       'probas', 'urls', 'phase'],\n",
      "      dtype='object')\n",
      "Processing chunk with 100000 rows\n",
      "Index(['quoteID', 'quotation', 'speaker', 'qids', 'date', 'numOccurrences',\n",
      "       'probas', 'urls', 'phase'],\n",
      "      dtype='object')\n",
      "Processing chunk with 100000 rows\n",
      "Index(['quoteID', 'quotation', 'speaker', 'qids', 'date', 'numOccurrences',\n",
      "       'probas', 'urls', 'phase'],\n",
      "      dtype='object')\n",
      "Processing chunk with 100000 rows\n",
      "Index(['quoteID', 'quotation', 'speaker', 'qids', 'date', 'numOccurrences',\n",
      "       'probas', 'urls', 'phase'],\n",
      "      dtype='object')\n",
      "Processing chunk with 100000 rows\n",
      "Index(['quoteID', 'quotation', 'speaker', 'qids', 'date', 'numOccurrences',\n",
      "       'probas', 'urls', 'phase'],\n",
      "      dtype='object')\n",
      "Processing chunk with 100000 rows\n",
      "Index(['quoteID', 'quotation', 'speaker', 'qids', 'date', 'numOccurrences',\n",
      "       'probas', 'urls', 'phase'],\n",
      "      dtype='object')\n",
      "Processing chunk with 100000 rows\n",
      "Index(['quoteID', 'quotation', 'speaker', 'qids', 'date', 'numOccurrences',\n",
      "       'probas', 'urls', 'phase'],\n",
      "      dtype='object')\n",
      "Processing chunk with 100000 rows\n",
      "Index(['quoteID', 'quotation', 'speaker', 'qids', 'date', 'numOccurrences',\n",
      "       'probas', 'urls', 'phase'],\n",
      "      dtype='object')\n",
      "Processing chunk with 100000 rows\n",
      "Index(['quoteID', 'quotation', 'speaker', 'qids', 'date', 'numOccurrences',\n",
      "       'probas', 'urls', 'phase'],\n",
      "      dtype='object')\n",
      "Processing chunk with 100000 rows\n",
      "Index(['quoteID', 'quotation', 'speaker', 'qids', 'date', 'numOccurrences',\n",
      "       'probas', 'urls', 'phase'],\n",
      "      dtype='object')\n",
      "Processing chunk with 100000 rows\n",
      "Index(['quoteID', 'quotation', 'speaker', 'qids', 'date', 'numOccurrences',\n",
      "       'probas', 'urls', 'phase'],\n",
      "      dtype='object')\n",
      "Processing chunk with 100000 rows\n",
      "Index(['quoteID', 'quotation', 'speaker', 'qids', 'date', 'numOccurrences',\n",
      "       'probas', 'urls', 'phase'],\n",
      "      dtype='object')\n",
      "Processing chunk with 100000 rows\n",
      "Index(['quoteID', 'quotation', 'speaker', 'qids', 'date', 'numOccurrences',\n",
      "       'probas', 'urls', 'phase'],\n",
      "      dtype='object')\n",
      "Processing chunk with 100000 rows\n",
      "Index(['quoteID', 'quotation', 'speaker', 'qids', 'date', 'numOccurrences',\n",
      "       'probas', 'urls', 'phase'],\n",
      "      dtype='object')\n",
      "Processing chunk with 100000 rows\n",
      "Index(['quoteID', 'quotation', 'speaker', 'qids', 'date', 'numOccurrences',\n",
      "       'probas', 'urls', 'phase'],\n",
      "      dtype='object')\n",
      "Processing chunk with 100000 rows\n",
      "Index(['quoteID', 'quotation', 'speaker', 'qids', 'date', 'numOccurrences',\n",
      "       'probas', 'urls', 'phase'],\n",
      "      dtype='object')\n",
      "Processing chunk with 100000 rows\n",
      "Index(['quoteID', 'quotation', 'speaker', 'qids', 'date', 'numOccurrences',\n",
      "       'probas', 'urls', 'phase'],\n",
      "      dtype='object')\n",
      "Processing chunk with 100000 rows\n",
      "Index(['quoteID', 'quotation', 'speaker', 'qids', 'date', 'numOccurrences',\n",
      "       'probas', 'urls', 'phase'],\n",
      "      dtype='object')\n",
      "Processing chunk with 100000 rows\n",
      "Index(['quoteID', 'quotation', 'speaker', 'qids', 'date', 'numOccurrences',\n",
      "       'probas', 'urls', 'phase'],\n",
      "      dtype='object')\n",
      "Processing chunk with 100000 rows\n",
      "Index(['quoteID', 'quotation', 'speaker', 'qids', 'date', 'numOccurrences',\n",
      "       'probas', 'urls', 'phase'],\n",
      "      dtype='object')\n",
      "Processing chunk with 100000 rows\n",
      "Index(['quoteID', 'quotation', 'speaker', 'qids', 'date', 'numOccurrences',\n",
      "       'probas', 'urls', 'phase'],\n",
      "      dtype='object')\n",
      "Processing chunk with 100000 rows\n",
      "Index(['quoteID', 'quotation', 'speaker', 'qids', 'date', 'numOccurrences',\n",
      "       'probas', 'urls', 'phase'],\n",
      "      dtype='object')\n",
      "Processing chunk with 100000 rows\n",
      "Index(['quoteID', 'quotation', 'speaker', 'qids', 'date', 'numOccurrences',\n",
      "       'probas', 'urls', 'phase'],\n",
      "      dtype='object')\n",
      "Processing chunk with 100000 rows\n",
      "Index(['quoteID', 'quotation', 'speaker', 'qids', 'date', 'numOccurrences',\n",
      "       'probas', 'urls', 'phase'],\n",
      "      dtype='object')\n",
      "Processing chunk with 100000 rows\n",
      "Index(['quoteID', 'quotation', 'speaker', 'qids', 'date', 'numOccurrences',\n",
      "       'probas', 'urls', 'phase'],\n",
      "      dtype='object')\n",
      "Processing chunk with 100000 rows\n",
      "Index(['quoteID', 'quotation', 'speaker', 'qids', 'date', 'numOccurrences',\n",
      "       'probas', 'urls', 'phase'],\n",
      "      dtype='object')\n",
      "Processing chunk with 100000 rows\n",
      "Index(['quoteID', 'quotation', 'speaker', 'qids', 'date', 'numOccurrences',\n",
      "       'probas', 'urls', 'phase'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk with 100000 rows\n",
      "Index(['quoteID', 'quotation', 'speaker', 'qids', 'date', 'numOccurrences',\n",
      "       'probas', 'urls', 'phase'],\n",
      "      dtype='object')\n",
      "Processing chunk with 100000 rows\n",
      "Index(['quoteID', 'quotation', 'speaker', 'qids', 'date', 'numOccurrences',\n",
      "       'probas', 'urls', 'phase'],\n",
      "      dtype='object')\n",
      "Processing chunk with 44449 rows\n",
      "Index(['quoteID', 'quotation', 'speaker', 'qids', 'date', 'numOccurrences',\n",
      "       'probas', 'urls', 'phase'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "positional indexers are out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_get_list_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1473\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1474\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_take_with_is_copy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1475\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_take_with_is_copy\u001b[1;34m(self, indices, axis)\u001b[0m\n\u001b[0;32m   3599\u001b[0m         \"\"\"\n\u001b[1;32m-> 3600\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3601\u001b[0m         \u001b[1;31m# Maybe set copy if we didn't actually change the index.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mtake\u001b[1;34m(self, indices, axis, is_copy, **kwargs)\u001b[0m\n\u001b[0;32m   3585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3586\u001b[1;33m         new_data = self._mgr.take(\n\u001b[0m\u001b[0;32m   3587\u001b[0m             \u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_block_manager_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverify\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mtake\u001b[1;34m(self, indexer, axis, verify, convert)\u001b[0m\n\u001b[0;32m   1466\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1467\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmaybe_convert_indices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1468\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexers.py\u001b[0m in \u001b[0;36mmaybe_convert_indices\u001b[1;34m(indices, n)\u001b[0m\n\u001b[0;32m    264\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 265\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"indices are out-of-bounds\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    266\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: indices are out-of-bounds",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-da064842f145>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'quotes-2020.json.bz2'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'bz2'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100000\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mdf_reader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdf_reader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mprocess_chunk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_filter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-da064842f145>\u001b[0m in \u001b[0;36mprocess_chunk\u001b[1;34m(chunk)\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0msims\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msims\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mdf_filter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchunk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msims\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    893\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1490\u001b[0m         \u001b[1;31m# a list of integers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1491\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mis_list_like_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1492\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_list_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1493\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1494\u001b[0m         \u001b[1;31m# a single integer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_get_list_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1475\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1476\u001b[0m             \u001b[1;31m# re-raise with different error message\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1477\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"positional indexers are out-of-bounds\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1478\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1479\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: positional indexers are out-of-bounds"
     ]
    }
   ],
   "source": [
    "df_filter = pd.DataFrame()\n",
    "\n",
    "def process_chunk(chunk):\n",
    "        print(f'Processing chunk with {len(chunk)} rows')\n",
    "        print(chunk.columns)\n",
    "        \n",
    "        # On cherche les 10 documents les plus similaires à celui-ci. \n",
    "        tokens = \"Climate change is a big problem.\".split()\n",
    "        new_vector = model.infer_vector(tokens)\n",
    "        sims = model.dv.most_similar([new_vector])\n",
    "    \n",
    "        sims = np.array(sims)\n",
    "        sims = sims[:,0]\n",
    "        sims = sims.astype(int)\n",
    "\n",
    "        df_filter = chunk.loc[sims]\n",
    "        \n",
    "\n",
    "with pd.read_json('quotes-2020.json.bz2', lines=True, compression='bz2', chunksize=100000) as df_reader:\n",
    "    for chunk in df_reader:\n",
    "        process_chunk(chunk)\n",
    "        \n",
    "print(df_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf59e71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
