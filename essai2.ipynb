{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d55637a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9354e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk with 100000 rows\n",
      "Index(['quoteID', 'quotation', 'speaker', 'qids', 'date', 'numOccurrences',\n",
      "       'probas', 'urls', 'phase'],\n",
      "      dtype='object')\n",
      "[(11190, 0.7680513858795166), (91448, 0.7215105891227722), (8558, 0.7070029377937317), (86148, 0.702319324016571), (92592, 0.6939815282821655), (40799, 0.6926105618476868), (10317, 0.6893508434295654), (17129, 0.6839941740036011), (76454, 0.681468665599823), (29958, 0.6807796359062195)]\n",
      "<class 'list'>\n",
      "<class 'numpy.ndarray'>\n",
      "[11190 91448  8558 86148 92592 40799 10317 17129 76454 29958]\n",
      "Processing chunk with 100000 rows\n",
      "Index(['quoteID', 'quotation', 'speaker', 'qids', 'date', 'numOccurrences',\n",
      "       'probas', 'urls', 'phase'],\n",
      "      dtype='object')\n",
      "[(37942, 0.8083097338676453), (96891, 0.7581177949905396), (97327, 0.7416934370994568), (24263, 0.7303981184959412), (86447, 0.7205579876899719), (4347, 0.7029721140861511), (20135, 0.7010608911514282), (4323, 0.6997730135917664), (81043, 0.6848158240318298), (37964, 0.6819020509719849)]\n",
      "<class 'list'>\n",
      "<class 'numpy.ndarray'>\n",
      "[37942 96891 97327 24263 86447  4347 20135  4323 81043 37964]\n",
      "Processing chunk with 100000 rows\n",
      "Index(['quoteID', 'quotation', 'speaker', 'qids', 'date', 'numOccurrences',\n",
      "       'probas', 'urls', 'phase'],\n",
      "      dtype='object')\n",
      "[(90229, 0.7980241179466248), (99346, 0.7772613763809204), (624, 0.7660133242607117), (86122, 0.7490965723991394), (91647, 0.7445955872535706), (96167, 0.7388250231742859), (61415, 0.7354418635368347), (91680, 0.7253167033195496), (20070, 0.7197405695915222), (36454, 0.7160822153091431)]\n",
      "<class 'list'>\n",
      "<class 'numpy.ndarray'>\n",
      "[90229 99346   624 86122 91647 96167 61415 91680 20070 36454]\n",
      "Processing chunk with 100000 rows\n",
      "Index(['quoteID', 'quotation', 'speaker', 'qids', 'date', 'numOccurrences',\n",
      "       'probas', 'urls', 'phase'],\n",
      "      dtype='object')\n",
      "[(10016, 0.8489370346069336), (76128, 0.7993277311325073), (79999, 0.7523990869522095), (12055, 0.7499338388442993), (31538, 0.7450806498527527), (20278, 0.736282467842102), (79365, 0.7327501773834229), (36959, 0.7195619940757751), (85368, 0.7140383124351501), (502, 0.7131309509277344)]\n",
      "<class 'list'>\n",
      "<class 'numpy.ndarray'>\n",
      "[10016 76128 79999 12055 31538 20278 79365 36959 85368   502]\n",
      "Processing chunk with 100000 rows\n",
      "Index(['quoteID', 'quotation', 'speaker', 'qids', 'date', 'numOccurrences',\n",
      "       'probas', 'urls', 'phase'],\n",
      "      dtype='object')\n",
      "[(32661, 0.7107306718826294), (44500, 0.7034931182861328), (25181, 0.6987009048461914), (45346, 0.6974579691886902), (74935, 0.692718505859375), (81314, 0.6918591260910034), (72881, 0.6894509792327881), (95235, 0.6868132948875427), (74462, 0.6868044137954712), (46780, 0.6842301487922668)]\n",
      "<class 'list'>\n",
      "<class 'numpy.ndarray'>\n",
      "[32661 44500 25181 45346 74935 81314 72881 95235 74462 46780]\n",
      "Processing chunk with 100000 rows\n",
      "Index(['quoteID', 'quotation', 'speaker', 'qids', 'date', 'numOccurrences',\n",
      "       'probas', 'urls', 'phase'],\n",
      "      dtype='object')\n",
      "[(3198, 0.7317891120910645), (84130, 0.7291567325592041), (28457, 0.7283557057380676), (34810, 0.7198088765144348), (7067, 0.7120524644851685), (90806, 0.7105959057807922), (77269, 0.7100334763526917), (79257, 0.7070098519325256), (89152, 0.70027756690979), (4895, 0.6952561736106873)]\n",
      "<class 'list'>\n",
      "<class 'numpy.ndarray'>\n",
      "[ 3198 84130 28457 34810  7067 90806 77269 79257 89152  4895]\n",
      "Processing chunk with 100000 rows\n",
      "Index(['quoteID', 'quotation', 'speaker', 'qids', 'date', 'numOccurrences',\n",
      "       'probas', 'urls', 'phase'],\n",
      "      dtype='object')\n",
      "[(9660, 0.7838336825370789), (28104, 0.7695395350456238), (65536, 0.7490537166595459), (1514, 0.7461069822311401), (47361, 0.7323011159896851), (98948, 0.7127665877342224), (85313, 0.7048289179801941), (98738, 0.7019368410110474), (77144, 0.6988372802734375), (7914, 0.6983188986778259)]\n",
      "<class 'list'>\n",
      "<class 'numpy.ndarray'>\n",
      "[ 9660 28104 65536  1514 47361 98948 85313 98738 77144  7914]\n",
      "Processing chunk with 100000 rows\n",
      "Index(['quoteID', 'quotation', 'speaker', 'qids', 'date', 'numOccurrences',\n",
      "       'probas', 'urls', 'phase'],\n",
      "      dtype='object')\n",
      "[(45376, 0.7369228005409241), (42429, 0.7114675045013428), (11672, 0.704444169998169), (12395, 0.691444993019104), (28419, 0.6875125765800476), (5524, 0.6852489113807678), (68758, 0.6842358112335205), (47507, 0.6791868209838867), (76801, 0.6769253015518188), (69662, 0.6748517751693726)]\n",
      "<class 'list'>\n",
      "<class 'numpy.ndarray'>\n",
      "[45376 42429 11672 12395 28419  5524 68758 47507 76801 69662]\n",
      "Processing chunk with 100000 rows\n",
      "Index(['quoteID', 'quotation', 'speaker', 'qids', 'date', 'numOccurrences',\n",
      "       'probas', 'urls', 'phase'],\n",
      "      dtype='object')\n",
      "[(22709, 0.7941672205924988), (73873, 0.7804458141326904), (35399, 0.7775828838348389), (16841, 0.7715970873832703), (81261, 0.7466741800308228), (90697, 0.7418715953826904), (59481, 0.7405278086662292), (24409, 0.7297009229660034), (98826, 0.7295238375663757), (27938, 0.7177941799163818)]\n",
      "<class 'list'>\n",
      "<class 'numpy.ndarray'>\n",
      "[22709 73873 35399 16841 81261 90697 59481 24409 98826 27938]\n",
      "Processing chunk with 100000 rows\n",
      "Index(['quoteID', 'quotation', 'speaker', 'qids', 'date', 'numOccurrences',\n",
      "       'probas', 'urls', 'phase'],\n",
      "      dtype='object')\n",
      "[(5368, 0.7910547256469727), (60707, 0.7421037554740906), (82991, 0.727319598197937), (95335, 0.7208300828933716), (69356, 0.7053115963935852), (13077, 0.702250063419342), (79402, 0.6930129528045654), (18903, 0.6865808963775635), (97004, 0.683306097984314), (91381, 0.6770606637001038)]\n",
      "<class 'list'>\n",
      "<class 'numpy.ndarray'>\n",
      "[ 5368 60707 82991 95335 69356 13077 79402 18903 97004 91381]\n",
      "Processing chunk with 100000 rows\n",
      "Index(['quoteID', 'quotation', 'speaker', 'qids', 'date', 'numOccurrences',\n",
      "       'probas', 'urls', 'phase'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df_filter = pd.DataFrame()\n",
    "\n",
    "def process_chunk(chunk):\n",
    "        print(f'Processing chunk with {len(chunk)} rows')\n",
    "        print(chunk.columns)\n",
    "\n",
    "        vocab_quotes = chunk.quotation.apply(gensim.utils.simple_preprocess)\n",
    "        tagged_data_2 = [TaggedDocument(words=doc, tags=[i]) for i, doc in enumerate(vocab_quotes)]\n",
    "        \n",
    "        # On définit le modèle, il y a beaucoup de paramètres à étudier pour optimiser\n",
    "        #model = gensim.models.Doc2Vec(vector_size=300, window=10, min_count=5, workers=4, epochs=20)\n",
    "        model = gensim.models.Doc2Vec(vector_size=100, dbow_words= 1, dm=0,  window=5, seed=1337, min_count=5, workers=4,\n",
    "                              alpha=0.025, min_alpha=0.025)\n",
    "        \n",
    "        # On construit le vocab nécessaire au training \n",
    "        model.build_vocab(tagged_data_2)\n",
    "        \n",
    "        # On entraine le modèle\n",
    "        model.train(tagged_data_2, total_examples=model.corpus_count, epochs=20)\n",
    "        \n",
    "        # On cherche les 10 documents les plus similaires à celui-ci. \n",
    "        tokens = \"Climate change is a big problem.\".split()\n",
    "        new_vector = model.infer_vector(tokens)\n",
    "        sims = model.dv.most_similar([new_vector])\n",
    "        print(sims)\n",
    "        \n",
    "        print(type(sims))\n",
    "        sims = np.array(sims)\n",
    "        sims = sims[:,0]\n",
    "        sims = sims.astype(int)\n",
    "        \n",
    "        print(type(sims))\n",
    "        print(sims)\n",
    "        \n",
    "        df_filter = chunk.iloc[sims]\n",
    "        \n",
    "\n",
    "with pd.read_json('quotes-2020.json.bz2', lines=True, compression='bz2', chunksize=100000) as df_reader:\n",
    "    for chunk in df_reader:\n",
    "        process_chunk(chunk)\n",
    "        \n",
    "print(df_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc0b6bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3078ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
